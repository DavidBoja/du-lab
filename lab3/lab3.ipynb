{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadatak 1: učitavanje podataka i batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset():\n",
    "    \n",
    "    def __init__(self, input_file, batch_size, sequence_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.preprocess(input_file)\n",
    "    \n",
    "    def preprocess(self, input_file):\n",
    "        with open(input_file, 'r') as f:\n",
    "            data = f.read() # .decode('utf-8') <- python 2\n",
    "\n",
    "        # count and sort most frequent characters\n",
    "        char_counter = collections.Counter(data)\n",
    "        self.sorted_chars = list(zip(*char_counter.most_common()))[0]\n",
    "\n",
    "        # self.sorted_chars contains just the characters ordered descending by frequency\n",
    "        self.char2id = dict(zip(self.sorted_chars, range(len(self.sorted_chars))))\n",
    "        # reverse the mapping\n",
    "        self.id2char = {k:v for v,k in self.char2id.items()}\n",
    "        # convert the data to ids\n",
    "        self.x = np.array(list(map(self.char2id.get, data)))\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        # returns the sequence encoded as integers\n",
    "        return np.array(list(map(self.char2id.get, sequence)))\n",
    "\n",
    "    def decode(self, encoded_sequence):\n",
    "        # returns the sequence decoded as letters\n",
    "        return ''.join(list(map(self.id2char.get, encoded_sequence)))\n",
    "    \n",
    "    def create_minibatches(self):\n",
    "        self.i = 0\n",
    "        self.num_batches = int(len(self.x) / (self.batch_size * self.sequence_length)) # calculate the number of batches\n",
    "\n",
    "        #######################################\n",
    "        #       Convert data to batches       #\n",
    "        #######################################\n",
    "\n",
    "        num_elements = self.num_batches * self.batch_size * self.sequence_length\n",
    "        \n",
    "        crop_x = self.x[:num_elements]\n",
    "        self.minibatches_x = crop_x.reshape(self.num_batches, self.batch_size, self.sequence_length)\n",
    "        \n",
    "        crop_y = self.x[1:num_elements + 1]\n",
    "        self.minibatches_y = crop_y.reshape(self.num_batches, self.batch_size, self.sequence_length)\n",
    "    \n",
    "    def next_minibatch(self):\n",
    "        # handling batch pointer & reset\n",
    "        # new_epoch is a boolean indicating if the batch pointer was reset\n",
    "        # in this function call\n",
    "        \n",
    "        batch_x, batch_y = self.minibatches_x[self.i], self.minibatches_y[self.i]\n",
    "        new_epoch = self.i == 0\n",
    "        \n",
    "        self.i += 1\n",
    "        if self.i == self.num_batches:\n",
    "            self.i = 0\n",
    "        \n",
    "        return new_epoch, batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: Test123\n",
      "Encoded: [32  1  9  2 59 60 66]\n",
      "Decoded: Test123\n",
      "First two sequences of x: CORNELIUS:\n",
      "I can't pretend to  be your husband... David's in \n",
      "First two sequences of y: ORNELIUS:\n",
      "I can't pretend to b e your husband... David's in g\n"
     ]
    }
   ],
   "source": [
    "input_file = 'data/selected_conversations.txt'\n",
    "batch_size = 100\n",
    "sequence_length = 30\n",
    "\n",
    "dataset = TextDataset(input_file, batch_size, sequence_length)\n",
    "\n",
    "raw = 'Test123'\n",
    "print('Raw:', raw)\n",
    "\n",
    "encoded = dataset.encode(raw)\n",
    "print('Encoded:', encoded)\n",
    "\n",
    "decoded = dataset.decode(encoded)\n",
    "print('Decoded:', decoded)\n",
    "\n",
    "dataset.create_minibatches()\n",
    "_, batch_x, batch_y = dataset.next_minibatch()\n",
    "print('First two sequences of x:', dataset.decode(batch_x[0]), dataset.decode(batch_x[1]))\n",
    "print('First two sequences of y:', dataset.decode(batch_y[0]), dataset.decode(batch_y[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadatak 2: obična jednoslojna povratna neuronska mreža"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    \n",
    "    def __init__(self, hidden_size, sequence_length, vocab_size, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        def norm_init(size):\n",
    "            loc = 0\n",
    "            scale = 1e-2\n",
    "            return np.random.normal(loc=loc, scale=scale, size=size)\n",
    "\n",
    "        self.U = norm_init((self.vocab_size, self.hidden_size)) # ... input projection\n",
    "        self.W = norm_init((self.hidden_size, self.hidden_size)) # ... hidden-to-hidden projection\n",
    "        self.b = np.zeros((self.hidden_size, 1)) # ... input bias\n",
    "\n",
    "        self.V = norm_init((self.hidden_size, self.vocab_size)) # ... output projection\n",
    "        self.c = np.zeros((self.vocab_size, 1)) # ... output bias\n",
    "\n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U, self.memory_W, self.memory_V = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        self.memory_b, self.memory_c = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        \n",
    "    def rnn_step_forward(self, x, h_prev, U, W, b):\n",
    "        # A single time step forward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # x - input data (minibatch size x input dimension)\n",
    "        # h_prev - previous hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "        \n",
    "        h_current = np.tanh(np.dot(h_prev, W) + np.dot(x, U) + b.transpose())\n",
    "        cache = (h_current, W, h_prev, x)\n",
    "\n",
    "        # return the new hidden state and a tuple of values needed for the backward step\n",
    "        \n",
    "        return h_current, cache\n",
    "\n",
    "\n",
    "    def rnn_forward(self, x, h0, U, W, b):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        # x - input data for the whole time-series (minibatch size x sequence_length x input dimension)\n",
    "        # h0 - initial hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "        \n",
    "        hs, caches = [], []\n",
    "\n",
    "        h_prev = h0\n",
    "        for i in range(x.shape[1]):\n",
    "            h_current, cache = self.rnn_step_forward(x[:,i], h_prev, U, W, b)\n",
    "\n",
    "            hs.append(h_current)\n",
    "            caches.append(cache)\n",
    "            \n",
    "            h_prev = h_current\n",
    "\n",
    "        # return the hidden states for the whole time series (T+1)\n",
    "        # and a tuple of values needed for the backward step\n",
    "        \n",
    "        return np.stack(hs, axis=1), caches\n",
    "    \n",
    "    def rnn_step_backward(self, grad_next, cache):\n",
    "        # A single time step backward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # grad_next - upstream gradient of the loss with respect to the next hidden state and current output\n",
    "        # cache - cached information from the forward pass\n",
    "        \n",
    "        h_current, W, h_prev, x = cache\n",
    "        dtanh = grad_next * (1 - h_current**2)\n",
    "        \n",
    "        dh_prev = np.dot(dtanh, W)\n",
    "        dU = np.dot(x.transpose(), dtanh)\n",
    "        dW = np.dot(h_prev.transpose(), dtanh)\n",
    "        db = np.sum(dtanh, axis=0).reshape(-1, 1)\n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # HINT: you can use the chain rule to compute the derivative of the\n",
    "        # hyperbolic tangent function and use it to compute the gradient\n",
    "        # with respect to the remaining parameters\n",
    "        \n",
    "        return dh_prev, dU, dW, db\n",
    "\n",
    "    def rnn_backward(self, dh, cache):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        dU = np.zeros(self.U.shape)\n",
    "        dW = np.zeros(self.W.shape)\n",
    "        db = np.zeros(self.b.shape)\n",
    "        \n",
    "        dh_prev = 0\n",
    "        for i in reversed(range(self.sequence_length)):\n",
    "            grad_next = dh[:,i] + dh_prev\n",
    "            \n",
    "            dh_prev, dU_t, dW_t, db_t = self.rnn_step_backward(grad_next, cache[i])\n",
    "            \n",
    "            dU += dU_t / self.sequence_length\n",
    "            dW += dW_t / self.sequence_length\n",
    "            db += db_t / self.sequence_length\n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # for the whole time series.\n",
    "        # Why are we not computing the gradient with respect to inputs (x)?\n",
    "\n",
    "        return dU, dW, db\n",
    "    \n",
    "    def output(self, h, V, c):\n",
    "        # Calculate the output probabilities of the network\n",
    "        return np.dot(h, V) + c.transpose()\n",
    "\n",
    "    def output_loss_and_grads(self, h, V, c, y):\n",
    "        # Calculate the loss of the network for each of the outputs\n",
    "\n",
    "        # h - hidden states of the network for each timestep. \n",
    "        #     the dimensionality of h is (batch size x sequence length x hidden size (the initial state is irrelevant for the output)\n",
    "        # V - the output projection matrix of dimension hidden size x vocabulary size\n",
    "        # c - the output bias of dimension vocabulary size x 1\n",
    "        # y - the true class distribution - a one-hot vector of dimension \n",
    "        #     vocabulary size x 1 - you need to do this conversion prior to\n",
    "        #     passing the argument. A fast way to create a one-hot vector from\n",
    "        #     an id could be something like the following code:\n",
    "\n",
    "        #   y[timestep] = np.zeros((vocabulary_size, 1))\n",
    "        #   y[timestep][batch_y[timestep]] = 1\n",
    "\n",
    "        #     where y might be a dictionary.\n",
    "\n",
    "        batch_size = y.shape[0]\n",
    "        \n",
    "        loss = None\n",
    "        dh = np.zeros(self.W.shape)\n",
    "        dV = np.zeros(self.V.shape)\n",
    "        dc = None\n",
    "        \n",
    "        # calculate the output (o) - unnormalized log probabilities of classes\n",
    "        o = self.output(h, V, c)\n",
    "        \n",
    "        # calculate yhat - softmax of the output\n",
    "        yhat = np.exp(o) / np.sum(np.exp(o), axis=2, keepdims=True)\n",
    "        \n",
    "        # calculate the cross-entropy loss\n",
    "        loss = -np.sum(y * np.log(yhat)) / (batch_size * self.sequence_length)\n",
    "        \n",
    "        # calculate the derivative of the cross-entropy softmax loss with respect to the output (o)\n",
    "        do = yhat - y\n",
    "        \n",
    "        # calculate the gradients with respect to the output parameters V and c\n",
    "        for i in range(batch_size):\n",
    "            dV += np.dot(h[i].transpose(), do[i]) / self.sequence_length\n",
    "        \n",
    "        dc = np.sum(np.sum(do, axis=0) / batch_size, axis=0).reshape(-1, 1) / self.sequence_length\n",
    "        \n",
    "        # calculate the gradients with respect to the hidden layer h\n",
    "        dh = np.dot(do, V.transpose())\n",
    "        \n",
    "        return loss, dh, dV, dc\n",
    "    \n",
    "    # The inputs to the function are just indicative since the variables are mostly present as class properties\n",
    "    def update(self, dU, dW, db, dV, dc,\n",
    "               U, W, b, V, c,\n",
    "               memory_U, memory_W, memory_b, memory_V, memory_c):\n",
    "        a_min = -5\n",
    "        a_max = 5\n",
    "        for param, dparam, mem in zip([U, W, b, V, c],\n",
    "                                      [dU, dW, db, dV, dc],\n",
    "                                      [memory_U, memory_W, memory_b, memory_V, memory_c]):\n",
    "            dparam = np.clip(dparam, a_min, a_max)\n",
    "            # update memory matrices\n",
    "            mem += dparam * dparam\n",
    "            # perform the Adagrad update of parameters\n",
    "            param += -self.learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "    \n",
    "    def step(self, h0, x_oh, y_oh):\n",
    "        # Makes one forward and backward pass\n",
    "        \n",
    "        # h0 - initial hidden state (hidden_size x 1)\n",
    "        # x_oh - one-hot input (minibatch_size x sequence_length x vocab_size) \n",
    "        # y_oh - one-hot output (minibatch_size x sequence_length x vocab_size)\n",
    "        \n",
    "        h, cache = self.rnn_forward(x_oh, h0, self.U, self.W, self.b)\n",
    "        \n",
    "        loss, dh, dV, dc = self.output_loss_and_grads(h, self.V, self.c, y_oh)\n",
    "        \n",
    "        dU, dW, db = self.rnn_backward(dh, cache)\n",
    "        \n",
    "        self.update(dU, dW, db, dV, dc,\n",
    "                    self.U, self.W, self.b, self.V, self.c,\n",
    "                    self.memory_U, self.memory_W, self.memory_b, self.memory_V, self.memory_c)\n",
    "        \n",
    "        # return loss and last hidden state\n",
    "        return loss, h[:,-1]\n",
    "    \n",
    "    def sample(self, seed_onehot, n_sample):\n",
    "        res = []\n",
    "        \n",
    "        h0 = np.zeros((1, self.hidden_size))\n",
    "        # inicijalizirati h0 na vektor nula\n",
    "        # seed string pretvoriti u one-hot reprezentaciju ulaza\n",
    "\n",
    "        x_oh = seed_onehot.reshape(1, seed_onehot.shape[0], seed_onehot.shape[1])\n",
    "        h, _ = self.rnn_forward(x_oh, h0, self.U, self.W, self.b)\n",
    "        h0 = h[:,-1]\n",
    "        \n",
    "        o = self.output(h0, self.V, self.c)\n",
    "        yhat = np.exp(o) / np.sum(np.exp(o))\n",
    "        \n",
    "        encoded = np.random.choice(71, yhat)\n",
    "        res.append(encoded)\n",
    "        x_oh = np.zeros((1, 71))\n",
    "        x_oh[0][encoded] = 1\n",
    "        \n",
    "        for i in range(n_sample - 1):\n",
    "            h, _ = self.rnn_step_forward(x_oh, h0, self.U, self.W, self.b)\n",
    "        \n",
    "            o = self.output(h, self.V, self.c)\n",
    "            yhat = np.exp(o) / np.sum(np.exp(o))\n",
    "\n",
    "            encoded = np.random.choice(71, yhat)\n",
    "            res.append(encoded)\n",
    "            x_oh = np.zeros((1, 71))\n",
    "            x_oh[0][encoded] = 1\n",
    "            \n",
    "            h0 = h\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=100):\n",
    "    \n",
    "    vocab_size = len(dataset.sorted_chars)\n",
    "    rnn = RNN(hidden_size, sequence_length, vocab_size, learning_rate) # initialize the recurrent network\n",
    "\n",
    "    current_epoch = 0 \n",
    "    batch = 1\n",
    "\n",
    "    h0 = np.zeros((hidden_size, 1))\n",
    "\n",
    "    average_loss = 0\n",
    "\n",
    "    while current_epoch < max_epochs: \n",
    "        e, x, y = dataset.next_minibatch()\n",
    "        \n",
    "        if e: \n",
    "            current_epoch += 1\n",
    "            # h0 = np.zeros((hidden_size, 1))\n",
    "            h0 = np.zeros((batch_size, hidden_size))\n",
    "            # why do we reset the hidden state here?\n",
    "\n",
    "        def to_one_hot(encoded, sequence_length, vocab_size):\n",
    "            N = encoded.shape[0]\n",
    "            oh = np.zeros((N, sequence_length, vocab_size))\n",
    "            for i in range(N):\n",
    "                oh[i][range(sequence_length), encoded[i]] = 1\n",
    "            return oh\n",
    "        \n",
    "        # One-hot transform the x and y batches\n",
    "        x_oh, y_oh = to_one_hot(x, sequence_length, vocab_size), to_one_hot(y, sequence_length, vocab_size)\n",
    "\n",
    "        # Run the recurrent network on the current batch\n",
    "        # Since we are using windows of a short length of characters,\n",
    "        # the step function should return the hidden state at the end\n",
    "        # of the unroll. You should then use that hidden state as the\n",
    "        # input for the next minibatch. In this way, we artificially\n",
    "        # preserve context between batches.\n",
    "        loss, h0 = rnn.step(h0, x_oh, y_oh)\n",
    "\n",
    "        if batch % sample_every == 0:\n",
    "            print(batch, round(loss, 8))\n",
    "            # run sampling (2.2)\n",
    "            seed = 'HAN:\\nIs that good or bad?\\n\\n'\n",
    "            n_sample = 300\n",
    "            \n",
    "            def to_one_hot(encoded, vocab_size):\n",
    "                N = encoded.shape[0]\n",
    "                oh = np.zeros((N, vocab_size))\n",
    "                #oh[range(N)][encoded[range(N)]] = 1\n",
    "                for i in range(N):\n",
    "                    oh[i][encoded[i]] = 1\n",
    "                return oh\n",
    "            \n",
    "            seed_encoded = dataset.encode(seed)\n",
    "            seed_onehot = to_one_hot(seed_encoded, vocab_size)\n",
    "            \n",
    "            encoded = rnn.sample(seed_onehot, n_sample)\n",
    "            decoded = dataset.decode(encoded)\n",
    "            print(decoded)\n",
    "        \n",
    "        batch += 1\n",
    "        \n",
    "    print('test:')\n",
    "    seed = 'HAN:\\nIs that good or bad?\\n\\n'\n",
    "    n_sample = 300\n",
    "\n",
    "    def to_one_hot(encoded, vocab_size):\n",
    "        N = encoded.shape[0]\n",
    "        oh = np.zeros((N, vocab_size))\n",
    "        #oh[range(N)][encoded[range(N)]] = 1\n",
    "        for i in range(N):\n",
    "            oh[i][encoded[i]] = 1\n",
    "        return oh\n",
    "\n",
    "    seed_encoded = dataset.encode(seed)\n",
    "    seed_onehot = to_one_hot(seed_encoded, vocab_size)\n",
    "\n",
    "    encoded = rnn.sample(seed_onehot, n_sample)\n",
    "    decoded = dataset.decode(encoded)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 2.44752888\n",
      "JON:\n",
      "Whathe the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "500 2.13770887\n",
      "LERE:\n",
      "I I do the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "750 2.27179231\n",
      "DEFFREY:\n",
      "I do the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "1000 2.27927947\n",
      "LAND:\n",
      "Whe the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here th\n",
      "1250 2.18396961\n",
      "KOR:\n",
      "I the and the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the me the m\n",
      "1500 1.94310301\n",
      "MR. WHITE:\n",
      "What the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a cand the a can\n",
      "1750 2.17525626\n",
      "LUKE:\n",
      "I the the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be the be t\n",
      "2000 2.09905354\n",
      "DONNIE:\n",
      "I don't and the have the hat the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the\n",
      "2250 1.85900936\n",
      "LAN:\n",
      "I the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "2500 2.08275052\n",
      "ELELE:\n",
      "I don't the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the ha\n",
      "2750 2.01702348\n",
      "MR. WHITE:\n",
      "You don't the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for\n",
      "3000 1.9661981\n",
      "LUKE:\n",
      "I the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the \n",
      "3250 2.18101107\n",
      "DR. THAN:\n",
      "I the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have \n",
      "3500 1.83768722\n",
      "HAN:\n",
      "You do the have the hing the here the hing the here the hing the here the hing the here the hing the here the hing the here the hing the here the hing the here the hing the here the hing the here the hing the here the hing the here the hing the here the hing the here the hing the here the hing \n",
      "3750 1.9795312\n",
      "SARLELIE:\n",
      "I the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here \n",
      "4000 1.81535592\n",
      "MR. WHITE:\n",
      "I the was the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the st\n",
      "4250 1.9387969\n",
      "SARER:\n",
      "I don't was the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster\n",
      "4500 2.02443637\n",
      "DONZO:\n",
      "What the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have \n",
      "4750 1.98167978\n",
      "HAN:\n",
      "You do sore the hat the have the hat the have the hat the have the hat the have the hat the have the hat the have the hat the have the hat the have the hat the have the hat the have the hat the have the hat the have the hat the have the hat the have the hat the have the hat the have the hat the\n",
      "5000 2.14004501\n",
      "SARDY:\n",
      "What the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here \n",
      "5250 1.99237323\n",
      "MAVERICK:\n",
      "What the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the ha\n",
      "5500 2.05246979\n",
      "SARAH:\n",
      "I the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the have a the h\n",
      "5750 2.04902419\n",
      "DONZO:\n",
      "What the want the for the want the for the want the for the want the for the want the for the want the for the want the for the want the for the want the for the want the for the want the for the want the for the want the for the want the for the want the for the want the for the want the for\n",
      "6000 1.94601375\n",
      "DONDY:\n",
      "I don't was a the have the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the fo\n",
      "6250 2.03276851\n",
      "CHARAND:\n",
      "I the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have t\n",
      "6500 2.12732977\n",
      "MAVERICK:\n",
      "I do the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6750 2.11114211\n",
      "MANDY:\n",
      "I don't was a sting the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the\n",
      "7000 2.25665682\n",
      "PEN:\n",
      "What the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare \n",
      "7250 1.83360299\n",
      "CONDY:\n",
      "I don't the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the been the be\n",
      "7500 1.9598478\n",
      "DOROTHY:\n",
      "I the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have the have t\n",
      "7750 1.83188894\n",
      "DONN:\n",
      "I don't the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the fo\n",
      "8000 2.02437954\n",
      "FREDDY:\n",
      "I don't was the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare the stare \n",
      "8250 2.11910501\n",
      "DONZO:\n",
      "What the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck the fuck \n",
      "8500 1.94763676\n",
      "BETTY:\n",
      "I don't the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the for the f\n",
      "8750 1.81742439\n",
      "JEFFREY:\n",
      "You don't the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and t\n",
      "9000 2.1291267\n",
      "LAURA:\n",
      "You don't the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the here the \n",
      "9250 1.95640469\n",
      "FREDDY:\n",
      "I the was the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster and the ster \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bd2ca1d740f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msample_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrun_language_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-189fe88e4920>\u001b[0m in \u001b[0;36mrun_language_model\u001b[0;34m(dataset, max_epochs, hidden_size, sequence_length, learning_rate, sample_every)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# input for the next minibatch. In this way, we artificially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# preserve context between batches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msample_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8545e3e0991f>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, h0, x_oh, y_oh)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         self.update(dU, dW, db, dV, dc,\n",
      "\u001b[0;32m<ipython-input-4-8545e3e0991f>\u001b[0m in \u001b[0;36mrnn_backward\u001b[0;34m(self, dh, cache)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mgrad_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdh_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mdh_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdU_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_step_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mdU\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdU_t\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8545e3e0991f>\u001b[0m in \u001b[0;36mrnn_step_backward\u001b[0;34m(self, grad_next, cache)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mdh_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtanh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mdU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtanh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtanh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtanh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_file = 'data/selected_conversations.txt'\n",
    "batch_size = 32\n",
    "sequence_length = 30\n",
    "\n",
    "dataset = TextDataset(input_file, batch_size, sequence_length)\n",
    "dataset.create_minibatches()\n",
    "\n",
    "max_epochs = 50\n",
    "hidden_size = 100\n",
    "learning_rate = 5e-2\n",
    "sample_every = 250\n",
    "\n",
    "run_language_model(dataset, max_epochs, hidden_size, sequence_length, learning_rate, sample_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
