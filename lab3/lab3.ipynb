{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadatak 1: učitavanje podataka i batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextDataset():\n",
    "    \n",
    "    def __init__(self, input_file, batch_size, sequence_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.preprocess(input_file)\n",
    "    \n",
    "    def preprocess(self, input_file):\n",
    "        with open(input_file, 'r') as f:\n",
    "            data = f.read() # .decode('utf-8') <- python 2\n",
    "\n",
    "        # count and sort most frequent characters\n",
    "        char_counter = collections.Counter(data)\n",
    "        self.sorted_chars = list(zip(*char_counter.most_common()))[0]\n",
    "\n",
    "        # self.sorted_chars contains just the characters ordered descending by frequency\n",
    "        self.char2id = dict(zip(self.sorted_chars, range(len(self.sorted_chars))))\n",
    "        # reverse the mapping\n",
    "        self.id2char = {k:v for v,k in self.char2id.items()}\n",
    "        # convert the data to ids\n",
    "        self.x = np.array(list(map(self.char2id.get, data)))\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        # returns the sequence encoded as integers\n",
    "        return np.array(list(map(self.char2id.get, sequence)))\n",
    "\n",
    "    def decode(self, encoded_sequence):\n",
    "        # returns the sequence decoded as letters\n",
    "        return ''.join(list(map(self.id2char.get, encoded_sequence)))\n",
    "    \n",
    "    def create_minibatches(self):\n",
    "        self.i = 0\n",
    "        self.num_batches = int(len(self.x) / (self.batch_size * self.sequence_length)) # calculate the number of batches\n",
    "\n",
    "        #######################################\n",
    "        #       Convert data to batches       #\n",
    "        #######################################\n",
    "\n",
    "        num_elements = self.num_batches * self.batch_size * self.sequence_length\n",
    "        \n",
    "        crop_x = self.x[:num_elements]\n",
    "        self.minibatches_x = crop_x.reshape(self.num_batches, self.batch_size, self.sequence_length)\n",
    "        \n",
    "        crop_y = self.x[1:num_elements + 1]\n",
    "        self.minibatches_y = crop_y.reshape(self.num_batches, self.batch_size, self.sequence_length)\n",
    "    \n",
    "    def next_minibatch(self):\n",
    "        # handling batch pointer & reset\n",
    "        # new_epoch is a boolean indicating if the batch pointer was reset\n",
    "        # in this function call\n",
    "        \n",
    "        batch_x, batch_y = self.minibatches_x[self.i], self.minibatches_y[self.i]\n",
    "        new_epoch = self.i == 0\n",
    "        \n",
    "        self.i += 1\n",
    "        if self.i == self.num_batches:\n",
    "            self.i = 0\n",
    "        \n",
    "        return new_epoch, batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: Test123\n",
      "Encoded: [32  1  9  2 59 60 66]\n",
      "Decoded: Test123\n",
      "First two sequences of x: CORNELIUS:\n",
      "I can't pretend to  be your husband... David's in \n",
      "First two sequences of y: ORNELIUS:\n",
      "I can't pretend to b e your husband... David's in g\n"
     ]
    }
   ],
   "source": [
    "input_file = 'data/selected_conversations.txt'\n",
    "batch_size = 100\n",
    "sequence_length = 30\n",
    "\n",
    "dataset = TextDataset(input_file, batch_size, sequence_length)\n",
    "\n",
    "raw = 'Test123'\n",
    "print('Raw:', raw)\n",
    "\n",
    "encoded = dataset.encode(raw)\n",
    "print('Encoded:', encoded)\n",
    "\n",
    "decoded = dataset.decode(encoded)\n",
    "print('Decoded:', decoded)\n",
    "\n",
    "dataset.create_minibatches()\n",
    "_, batch_x, batch_y = dataset.next_minibatch()\n",
    "print('First two sequences of x:', dataset.decode(batch_x[0]), dataset.decode(batch_x[1]))\n",
    "print('First two sequences of y:', dataset.decode(batch_y[0]), dataset.decode(batch_y[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadatak 2: obična jednoslojna povratna neuronska mreža"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    \n",
    "    def __init__(self, hidden_size, sequence_length, vocab_size, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        def norm_init(size):\n",
    "            loc = 1e-2\n",
    "            scale = 1e-2\n",
    "            return np.random.normal(loc=loc, scale=scale, size=size)\n",
    "\n",
    "        self.U = norm_init((self.vocab_size, self.hidden_size)) # ... input projection\n",
    "        self.W = norm_init((self.hidden_size, self.hidden_size)) # ... hidden-to-hidden projection\n",
    "        self.b = np.zeros((self.hidden_size, 1)) # ... input bias\n",
    "\n",
    "        self.V = norm_init((self.hidden_size, self.vocab_size)) # ... output projection\n",
    "        self.c = np.zeros((self.vocab_size, 1)) # ... output bias\n",
    "\n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U, self.memory_W, self.memory_V = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        self.memory_b, self.memory_c = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        \n",
    "    def rnn_step_forward(self, x, h_prev, U, W, b):\n",
    "        # A single time step forward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # x - input data (minibatch size x input dimension)\n",
    "        # h_prev - previous hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "        \n",
    "        h = np.tanh(np.dot(h_prev, W) + np.dot(x, U) + b.transpose())\n",
    "        cache = (h, W, h_prev, x)\n",
    "\n",
    "        # return the new hidden state and a tuple of values needed for the backward step\n",
    "        return h, cache\n",
    "\n",
    "\n",
    "    def rnn_forward(self, x, h0, U, W, b):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        # x - input data for the whole time-series (minibatch size x sequence_length x input dimension)\n",
    "        # h0 - initial hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "        \n",
    "        h, cache = [], []\n",
    "\n",
    "        h_prev = h0\n",
    "        for i in range(x.shape[1]):\n",
    "            h_t, cache_t = self.rnn_step_forward(x[:,i], h_prev, U, W, b)\n",
    "\n",
    "            h.append(h_t)\n",
    "            cache.append(cache_t)\n",
    "            \n",
    "            h_prev = h_t\n",
    "\n",
    "        # return the hidden states for the whole time series (T+1)\n",
    "        # and a tuple of values needed for the backward step\n",
    "        return np.stack(h, axis=1), cache\n",
    "    \n",
    "    def rnn_step_backward(self, grad_next, cache):\n",
    "        # A single time step backward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # grad_next - upstream gradient of the loss with respect to the next hidden state and current output\n",
    "        # cache - cached information from the forward pass\n",
    "        \n",
    "        h, W, h_prev, x = cache\n",
    "        dtanh = 1 - h**2\n",
    "        \n",
    "        dh_prev = np.dot(grad_next * dtanh, W)\n",
    "        dU = np.dot(x.transpose(), grad_next * dtanh)\n",
    "        dW = np.dot(h_prev.transpose(), grad_next * dtanh)\n",
    "        db = np.sum(grad_next * dtanh, axis=0).reshape(-1, 1)\n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # HINT: you can use the chain rule to compute the derivative of the\n",
    "        # hyperbolic tangent function and use it to compute the gradient\n",
    "        # with respect to the remaining parameters\n",
    "        return dh_prev, dU, dW, db\n",
    "\n",
    "\n",
    "    def rnn_backward(self, dh, cache):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        dU = np.zeros(self.U.shape)\n",
    "        dW = np.zeros(self.W.shape)\n",
    "        db = np.zeros(self.b.shape)\n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # for the whole time series.\n",
    "        # Why are we not computing the gradient with respect to inputs (x)?\n",
    "        \n",
    "        dh_prev = 0\n",
    "        for i in reversed(range(self.sequence_length)):\n",
    "            grad_next = dh[:,i] + dh_prev\n",
    "            \n",
    "            dh_prev, dU_t, dW_t, db_t = self.rnn_step_backward(grad_next, cache[i])\n",
    "            \n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "            db += db_t\n",
    "\n",
    "        return dU, dW, db\n",
    "    \n",
    "    def output(self, h, V, c):\n",
    "        # Calculate the output probabilities of the network\n",
    "        return np.dot(h, V) + c.transpose()\n",
    "\n",
    "    def output_loss_and_grads(self, h, V, c, y):\n",
    "        # Calculate the loss of the network for each of the outputs\n",
    "\n",
    "        # h - hidden states of the network for each timestep. \n",
    "        #     the dimensionality of h is (batch size x sequence length x hidden size (the initial state is irrelevant for the output)\n",
    "        # V - the output projection matrix of dimension hidden size x vocabulary size\n",
    "        # c - the output bias of dimension vocabulary size x 1\n",
    "        # y - the true class distribution - a one-hot vector of dimension \n",
    "        #     vocabulary size x 1 - you need to do this conversion prior to\n",
    "        #     passing the argument. A fast way to create a one-hot vector from\n",
    "        #     an id could be something like the following code:\n",
    "\n",
    "        #   y[timestep] = np.zeros((vocabulary_size, 1))\n",
    "        #   y[timestep][batch_y[timestep]] = 1\n",
    "\n",
    "        #     where y might be a dictionary.\n",
    "\n",
    "        batch_size = y.shape[0]\n",
    "        \n",
    "        loss = None\n",
    "        dh = np.zeros(self.W.shape)\n",
    "        dV = np.zeros(self.V.shape)\n",
    "        dc = None\n",
    "        \n",
    "        # calculate the output (o) - unnormalized log probabilities of classes\n",
    "        o = self.output(h, V, c)\n",
    "        \n",
    "        # calculate yhat - softmax of the output\n",
    "        yhat = np.exp(o) / np.sum(np.exp(o), axis=2, keepdims=True)\n",
    "        \n",
    "        # calculate the cross-entropy loss\n",
    "        loss = -np.sum(y * np.log(yhat)) / (batch_size * self.sequence_length)\n",
    "        \n",
    "        # calculate the derivative of the cross-entropy softmax loss with respect to the output (o)\n",
    "        do = yhat - y\n",
    "        \n",
    "        # calculate the gradients with respect to the output parameters V and c\n",
    "        for i in range(batch_size):\n",
    "            dV += np.dot(h[i].transpose(), do[i])\n",
    "        \n",
    "        dc = np.sum(np.sum(do, axis=0) / batch_size, axis=0).reshape(-1, 1)\n",
    "        \n",
    "        # calculate the gradients with respect to the hidden layer h\n",
    "        dh = np.dot(do, V.transpose())\n",
    "        \n",
    "        return loss, dh, dV, dc\n",
    "    \n",
    "    # The inputs to the function are just indicative since the variables are mostly present as class properties\n",
    "    def update(self, dU, dW, db, dV, dc,\n",
    "               U, W, b, V, c,\n",
    "               memory_U, memory_W, memory_b, memory_V, memory_c):\n",
    "        a_min = -5\n",
    "        a_max = 5\n",
    "        for param, dparam, mem in zip([U, W, b, V, c],\n",
    "                                      [dU, dW, db, dV, dc],\n",
    "                                      [memory_U, memory_W, memory_b, memory_V, memory_c]):\n",
    "            dparam = np.clip(dparam, a_min, a_max)\n",
    "            # update memory matrices\n",
    "            mem += dparam * dparam\n",
    "            # perform the Adagrad update of parameters\n",
    "            param += -self.learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "    \n",
    "    def step(self, h0, x_oh, y_oh):\n",
    "        # Makes one forward and backward pass\n",
    "        \n",
    "        # h0 - initial hidden state (hidden_size x 1)\n",
    "        # x_oh - one-hot input (minibatch_size x sequence_length x vocab_size) \n",
    "        # y_oh - one-hot output (minibatch_size x sequence_length x vocab_size)\n",
    "        \n",
    "        h, cache = self.rnn_forward(x_oh, h0, self.U, self.W, self.b)\n",
    "        \n",
    "        loss, dh, dV, dc = self.output_loss_and_grads(h, self.V, self.c, y_oh)\n",
    "        \n",
    "        dU, dW, db = self.rnn_backward(dh, cache)\n",
    "        \n",
    "        self.update(dU, dW, db, dV, dc,\n",
    "                    self.U, self.W, self.b, self.V, self.c,\n",
    "                    self.memory_U, self.memory_W, self.memory_b, self.memory_V, self.memory_c)\n",
    "        \n",
    "        # return loss and last hidden state\n",
    "        return loss, h[:,-1]\n",
    "    \n",
    "    def sample(self, seed_onehot, n_sample):\n",
    "        res = []\n",
    "        \n",
    "        h0 = np.zeros((1, self.hidden_size))\n",
    "        # inicijalizirati h0 na vektor nula\n",
    "        # seed string pretvoriti u one-hot reprezentaciju ulaza\n",
    "\n",
    "        x_oh = seed_onehot.reshape(1, seed_onehot.shape[0], seed_onehot.shape[1])\n",
    "        h, _ = self.rnn_forward(x_oh, h0, self.U, self.W, self.b)\n",
    "        h0 = h[:,-1]\n",
    "        \n",
    "        o = self.output(h0, self.V, self.c)\n",
    "        yhat = np.exp(o) / np.sum(np.exp(o))\n",
    "        \n",
    "        encoded = np.argmax(yhat)\n",
    "        res.append(encoded)\n",
    "        x_oh = np.zeros((1, 71))\n",
    "        x_oh[0][encoded] = 1\n",
    "        \n",
    "        for i in range(n_sample - 1):\n",
    "            h, _ = self.rnn_step_forward(x_oh, h0, self.U, self.W, self.b)\n",
    "        \n",
    "            o = self.output(h, self.V, self.c)\n",
    "            yhat = np.exp(o) / np.sum(np.exp(o))\n",
    "\n",
    "            encoded = np.argmax(yhat)\n",
    "            res.append(encoded)\n",
    "            x_oh = np.zeros((1, 71))\n",
    "            x_oh[0][encoded] = 1\n",
    "            \n",
    "            h0 = h\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=100):\n",
    "    \n",
    "    vocab_size = len(dataset.sorted_chars)\n",
    "    rnn = RNN(hidden_size, sequence_length, vocab_size, learning_rate) # initialize the recurrent network\n",
    "\n",
    "    current_epoch = 0 \n",
    "    batch = 0\n",
    "\n",
    "    h0 = np.zeros((hidden_size, 1))\n",
    "\n",
    "    average_loss = 0\n",
    "\n",
    "    while current_epoch < max_epochs: \n",
    "        e, x, y = dataset.next_minibatch()\n",
    "        \n",
    "        if e: \n",
    "            current_epoch += 1\n",
    "            # h0 = np.zeros((hidden_size, 1))\n",
    "            h0 = np.zeros((batch_size, hidden_size))\n",
    "            # why do we reset the hidden state here?\n",
    "\n",
    "        def to_one_hot(encoded, sequence_length, vocab_size):\n",
    "            N = encoded.shape[0]\n",
    "            oh = np.zeros((N, sequence_length, vocab_size))\n",
    "            for i in range(N):\n",
    "                oh[i][range(sequence_length), encoded[i]] = 1\n",
    "            return oh\n",
    "        \n",
    "        # One-hot transform the x and y batches\n",
    "        x_oh, y_oh = to_one_hot(x, sequence_length, vocab_size), to_one_hot(y, sequence_length, vocab_size)\n",
    "\n",
    "        # Run the recurrent network on the current batch\n",
    "        # Since we are using windows of a short length of characters,\n",
    "        # the step function should return the hidden state at the end\n",
    "        # of the unroll. You should then use that hidden state as the\n",
    "        # input for the next minibatch. In this way, we artificially\n",
    "        # preserve context between batches.\n",
    "        loss, h0 = rnn.step(h0, x_oh, y_oh)\n",
    "\n",
    "        if batch % sample_every == 0:\n",
    "            print(batch, round(loss, 8))\n",
    "            # run sampling (2.2)\n",
    "            seed = 'HAN:\\nIs that good or bad?\\n\\n'\n",
    "            n_sample = 300\n",
    "            \n",
    "            def to_one_hot(encoded, vocab_size):\n",
    "                N = encoded.shape[0]\n",
    "                oh = np.zeros((N, vocab_size))\n",
    "                #oh[range(N)][encoded[range(N)]] = 1\n",
    "                for i in range(N):\n",
    "                    oh[i][encoded[i]] = 1\n",
    "                return oh\n",
    "            \n",
    "            seed_encoded = dataset.encode(seed)\n",
    "            seed_onehot = to_one_hot(seed_encoded, vocab_size)\n",
    "            \n",
    "            encoded = rnn.sample(seed_onehot, n_sample)\n",
    "            decoded = dataset.decode(encoded)\n",
    "            print(decoded)\n",
    "        \n",
    "        batch += 1\n",
    "        \n",
    "    print('test:')\n",
    "    seed = 'HAN:\\nIs that good or bad?\\n\\n'\n",
    "    n_sample = 300\n",
    "\n",
    "    def to_one_hot(encoded, vocab_size):\n",
    "        N = encoded.shape[0]\n",
    "        oh = np.zeros((N, vocab_size))\n",
    "        #oh[range(N)][encoded[range(N)]] = 1\n",
    "        for i in range(N):\n",
    "            oh[i][encoded[i]] = 1\n",
    "        return oh\n",
    "\n",
    "    seed_encoded = dataset.encode(seed)\n",
    "    seed_onehot = to_one_hot(seed_encoded, vocab_size)\n",
    "\n",
    "    encoded = rnn.sample(seed_onehot, n_sample)\n",
    "    decoded = dataset.decode(encoded)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.26320847\n",
      "LILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILILI\n",
      "300 2.22116085\n",
      "MANERE:\n",
      "What and at the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "600 2.17843483\n",
      "DORE:\n",
      "I we wan the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "900 2.16084504\n",
      "MAVERE:\n",
      "I wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wan the wa\n",
      "1200 2.15277638\n",
      "INE:\n",
      "I the want the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "1500 2.0491744\n",
      "LAND:\n",
      "I want the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "test:\n",
      "FREDDY:\n",
      "I the was the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n"
     ]
    }
   ],
   "source": [
    "input_file = 'data/selected_conversations.txt'\n",
    "batch_size = 100\n",
    "sequence_length = 30\n",
    "\n",
    "dataset = TextDataset(input_file, batch_size, sequence_length)\n",
    "dataset.create_minibatches()\n",
    "\n",
    "max_epochs = 10\n",
    "hidden_size = 100\n",
    "learning_rate = 5e-2\n",
    "sample_every = 300\n",
    "\n",
    "run_language_model(dataset, max_epochs, hidden_size, sequence_length, learning_rate, sample_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
